{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import docker \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from gym_torcs_docker import TorcsDockerEnv, obs_to_state\n",
    "from ddpg import DDPG\n",
    "from a3c import A3C\n",
    "\n",
    "docker_client = docker.from_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModelOnTrack(\n",
    "        docker_client, sess, model, trackname, max_steps=1000,\n",
    "        docker_port=3101):\n",
    "    \"\"\"Drives the model around the specified track for 1000 time steps\"\"\"\n",
    "\n",
    "    env = TorcsDockerEnv(\n",
    "        docker_client, 'test', port=docker_port)\n",
    "    observation = env.reset(relaunch=True)\n",
    "    state_t = obs_to_state(observation)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        action_t = model.predict(sess, state_t.reshape(1, state_t.shape[0]))\n",
    "        observation, reward_t, done, _ = env.step(action_t[0])\n",
    "        state_t = obs_to_state(observation)\n",
    "        results[i] = reward_t\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    env.end()\n",
    "\n",
    "    return results\n",
    "\n",
    "def testDDPG(docker_client, modeldir, test_tracks):\n",
    "    \"\"\"Loads the weights from the model dir and drives the agent around the provided test tracks\"\"\"\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    model = DDPG(docker_client)\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=5)\n",
    "    rewards = {}\n",
    "    with tf.Session(config=config) as sess:\n",
    "        ckpt = tf.train.get_checkpoint_state(modeldir)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        for track in test_tracks:\n",
    "            reward = testModelOnTrack(\n",
    "                docker_client, sess, model.actor, track, max_steps=1000,\n",
    "                docker_port=3121)\n",
    "            rewards[track] = reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_tracks = ['g-track-3', 'e-track-6', 'alpine-2']\n",
    "\n",
    "path_ddpg_ref = '../models/ddpg_ref'\n",
    "path_ddpg_1 = '../models/ddpg_1'\n",
    "path_ddpg_2 = '../models/ddpg_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/ddpg_ref/model-1050.cptk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-25 01:09:43,501] Restoring parameters from ../models/ddpg_ref/model-1050.cptk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for server on 3121............\n",
      "Waiting for server on 3121............\n",
      "Client connected on 3121..............\n",
      "-0.0518008636786\n",
      "0.462160325397\n",
      "1.11785491013\n",
      "1.66566063166\n",
      "2.00558318522\n",
      "2.46476364583\n",
      "2.68544453634\n",
      "3.00793015649\n",
      "3.25467007017\n",
      "3.48848456451\n",
      "3.62390686869\n",
      "3.55512679125\n",
      "3.67335994301\n",
      "3.54946736122\n",
      "3.51483001354\n",
      "3.28886603451\n",
      "3.12534135927\n",
      "2.88665936327\n",
      "2.55117902034\n",
      "2.23389734054\n",
      "1.90482252849\n",
      "1.55112077925\n",
      "1.19154577747\n",
      "0.827728243918\n",
      "0.43350507896\n",
      "0.0513089230392\n",
      "-0.327558419339\n",
      "-0.70684920611\n",
      "-1.0613514367\n",
      "-1.41003149142\n",
      "-1.72248503749\n",
      "-2.05891296285\n",
      "-2.39063411768\n",
      "-2.68772014806\n",
      "-2.92322908522\n",
      "-3.13159963831\n",
      "-3.38116760762\n",
      "-3.48844272593\n",
      "-3.73515456106\n",
      "-3.83965057713\n",
      "-3.87921706407\n",
      "-3.98104878571\n",
      "-4.04814462109\n",
      "-4.10673381768\n",
      "-4.16902851932\n",
      "-4.25179890045\n",
      "-4.22574613674\n",
      "-4.21480686854\n",
      "Waiting for server on 3121............\n",
      "Waiting for server on 3121............\n",
      "Client connected on 3121..............\n",
      "-0.0518008636786\n",
      "0.462160325397\n",
      "1.11785491013\n",
      "1.66566063166\n",
      "2.00558318522\n",
      "2.46476364583\n",
      "2.68544453634\n",
      "3.00793015649\n",
      "3.25467007017\n",
      "3.48848456451\n",
      "3.62390686869\n",
      "3.55512679125\n",
      "3.67335994301\n",
      "3.54946736122\n",
      "3.51483001354\n",
      "3.28886603451\n",
      "3.12534135927\n",
      "2.88665936327\n",
      "2.55117902034\n",
      "2.23389734054\n",
      "1.90482252849\n",
      "1.55112077925\n",
      "1.19154577747\n",
      "0.827728243918\n",
      "0.43350507896\n",
      "0.0513089230392\n",
      "-0.327558419339\n",
      "-0.70684920611\n",
      "-1.0613514367\n",
      "-1.41003149142\n",
      "-1.72248503749\n",
      "-2.05891296285\n",
      "-2.39063411768\n",
      "-2.68772014806\n",
      "-2.92322908522\n",
      "-3.13159963831\n",
      "-3.38116760762\n",
      "-3.48844272593\n",
      "-3.73515456106\n",
      "-3.83965057713\n",
      "-3.87921706407\n",
      "-3.98104878571\n",
      "-4.04814462109\n",
      "-4.10673381768\n",
      "-4.16902851932\n",
      "-4.25179890045\n",
      "-4.22574613674\n",
      "-4.21480686854\n",
      "Waiting for server on 3121............\n",
      "Waiting for server on 3121............\n",
      "Client connected on 3121..............\n",
      "-0.0518008636786\n",
      "0.462160325397\n",
      "1.11785491013\n",
      "1.66566063166\n",
      "2.00558318522\n",
      "2.46476364583\n",
      "2.68544453634\n",
      "3.00793015649\n",
      "3.25467007017\n",
      "3.48848456451\n",
      "3.62390686869\n",
      "3.55512679125\n",
      "3.67335994301\n",
      "3.54946736122\n",
      "3.51483001354\n",
      "3.28886603451\n",
      "3.12534135927\n",
      "2.88665936327\n",
      "2.55117902034\n",
      "2.23389734054\n",
      "1.90482252849\n",
      "1.55112077925\n",
      "1.19154577747\n",
      "0.827728243918\n",
      "0.43350507896\n",
      "0.0513089230392\n",
      "-0.327558419339\n",
      "-0.70684920611\n",
      "-1.0613514367\n",
      "-1.41003149142\n",
      "-1.72248503749\n",
      "-2.05891296285\n",
      "-2.39063411768\n",
      "-2.68772014806\n",
      "-2.92322908522\n",
      "-3.13159963831\n",
      "-3.38116760762\n",
      "-3.48844272593\n",
      "-3.73515456106\n",
      "-3.83965057713\n",
      "-3.87921706407\n",
      "-3.98104878571\n",
      "-4.04814462109\n",
      "-4.10673381768\n"
     ]
    }
   ],
   "source": [
    "ddpg_ref = testDDPG(docker_client, path_ddpg_ref, test_tracks)\n",
    "ddpg_1 = testDDPG(docker_client, path_ddpg_1, test_tracks)\n",
    "ddpg_2 = testDDPG(docker_client, path_ddpg_2, test_tracks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpine-2': -1, 'e-track-6': -1, 'g-track-3': -1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddpg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load networks.py\n",
    "import tensorflow as tf\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    HIDDEN1_UNITS = 300\n",
    "    HIDDEN2_UNITS = 600\n",
    "\n",
    "    def __init__(self, state_size, action_size, trainer):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.trainer = trainer\n",
    "        self.is_training = False\n",
    "\n",
    "\n",
    "class ActorCriticBaseNetwork(Network):\n",
    "\n",
    "    def __init__(self, state_size, action_size, trainer, tau):\n",
    "        super(ActorCriticBaseNetwork, self).__init__(\n",
    "            state_size, action_size, trainer)\n",
    "\n",
    "        self.tau = tau\n",
    "        self.weights = None\n",
    "        self.target_weights = None\n",
    "        self.cp_trgt_wgt_frm_wgt = None\n",
    "\n",
    "    def _create_target_train(self):\n",
    "        self.cp_trgt_wgt_frm_wgt = tf.group(\n",
    "            *[v1.assign(self.tau*v2 + (1-v1))\n",
    "              for v1, v2 in zip(self.target_weights, self.weights)])\n",
    "\n",
    "    def target_train(self, sess):\n",
    "        self.is_training = True\n",
    "        sess.run(self.cp_trgt_wgt_frm_wgt)\n",
    "\n",
    "\n",
    "class CriticNetwork(ActorCriticBaseNetwork):\n",
    "\n",
    "    def __init__(self, state_size, action_size, trainer, tau):\n",
    "\n",
    "        super(CriticNetwork, self).__init__(\n",
    "            state_size, action_size, trainer, tau)\n",
    "\n",
    "        self.net_scope = 'critic_network'\n",
    "        self.target_net_scope = 'target_critic_network'\n",
    "        # Now create the model\n",
    "        self.critic, self.weights, self.state, self.action = \\\n",
    "            self._create_network(self.net_scope)\n",
    "        self.target_critic, self.target_weights, self.target_state, \\\n",
    "            self.target_action = self._create_network(self.target_net_scope)\n",
    "        self._create_target_train()\n",
    "        # GRADIENTS for policy update\n",
    "        self.action_grads = tf.gradients(self.critic, self.action)\n",
    "        self.optimize, self.loss, self.expected_critic = self._create_train()\n",
    "\n",
    "    def _create_network(self, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "\n",
    "            state = tf.placeholder(\n",
    "                shape=[None, self.state_size], dtype=tf.float32, name='state')\n",
    "            action = tf.placeholder(\n",
    "                shape=[None, self.action_size],\n",
    "                dtype=tf.float32, name='action')\n",
    "\n",
    "            s_layer1 = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=state, activation=tf.nn.relu,\n",
    "                    units=CriticNetwork.HIDDEN1_UNITS),\n",
    "                training=self.is_training, name='s_layer_1')\n",
    "\n",
    "            s_layer2 = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=s_layer1,\n",
    "                    units=CriticNetwork.HIDDEN2_UNITS),\n",
    "                training=self.is_training, name='s_layer_2')\n",
    "\n",
    "            a_layer = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=action,\n",
    "                    units=CriticNetwork.HIDDEN2_UNITS),\n",
    "                training=self.is_training, name='a_layer')\n",
    "\n",
    "            c_layer = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=(s_layer2 + a_layer),\n",
    "                    activation=tf.nn.relu,\n",
    "                    units=CriticNetwork.HIDDEN2_UNITS),\n",
    "                training=self.is_training, name='c_layer')\n",
    "\n",
    "            critic = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(inputs=c_layer,\n",
    "                                units=self.action_size),\n",
    "                training=self.is_training, name='critic')\n",
    "\n",
    "            weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                        scope=scope)\n",
    "\n",
    "        return critic, weights, state, action\n",
    "\n",
    "    def _create_train(self):\n",
    "        expected_critic = tf.placeholder(shape=[None, self.action_size],\n",
    "                                         dtype=tf.float32,\n",
    "                                         name='expected_critic')\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(expected_critic-self.critic),\n",
    "                              name=\"loss\")\n",
    "\n",
    "        optimize = self.trainer.minimize(loss, name='optimize')\n",
    "\n",
    "        return optimize, loss, expected_critic\n",
    "\n",
    "    def target_predict(self, sess, states, actions):\n",
    "        self.is_training = False\n",
    "        return sess.run(\n",
    "            self.target_critic,\n",
    "            feed_dict={self.target_state: states,\n",
    "                       self.target_action: actions})\n",
    "\n",
    "    def gradients(self, sess, states, actions):\n",
    "        self.is_training = False\n",
    "        return sess.run(\n",
    "            self.action_grads,\n",
    "            feed_dict={self.state: states, self.action: actions})[0]\n",
    "\n",
    "    def train(self, sess, expected_critic, states, actions):\n",
    "        self.is_training = True\n",
    "        loss, _ = sess.run(\n",
    "            [self.loss, self.optimize],\n",
    "            feed_dict={\n",
    "                self.expected_critic: expected_critic, self.state: states,\n",
    "                self.action: actions})\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class ActorNetwork(ActorCriticBaseNetwork):\n",
    "\n",
    "    def __init__(self, state_size, action_size, trainer, tau):\n",
    "\n",
    "        super(ActorNetwork, self).__init__(\n",
    "            state_size, action_size, trainer, tau)\n",
    "\n",
    "        self.net_scope = 'actor_network'\n",
    "        self.target_net_scope = 'target_actor_network'\n",
    "        # Now create the model\n",
    "        self.action, self.weights, self.state = \\\n",
    "            self._create_network(self.net_scope)\n",
    "        self.target_action, self.target_weights, self.target_state = \\\n",
    "            self._create_network(self.target_net_scope)\n",
    "        self._create_target_train()\n",
    "        self.optimize, self.action_gradient = self._create_train()\n",
    "\n",
    "    def _create_network(self, scope):\n",
    "        with tf.variable_scope(scope):\n",
    "            state = tf.placeholder(tf.float32, [None, self.state_size],\n",
    "                                   name='state')\n",
    "\n",
    "            hidden0 = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=state, activation=tf.nn.relu,\n",
    "                    units=ActorNetwork.HIDDEN1_UNITS),\n",
    "                training=self.is_training, name='hidden_0')\n",
    "\n",
    "            hidden1 = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(inputs=hidden0, activation=tf.nn.relu,\n",
    "                                units=ActorNetwork.HIDDEN2_UNITS),\n",
    "                training=self.is_training, name='hidden_1')\n",
    "\n",
    "            steering = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=hidden1, units=1, activation=tf.nn.tanh),\n",
    "                training=self.is_training, name='steering')\n",
    "\n",
    "            acceleration = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=hidden1, units=1, activation=tf.nn.tanh),\n",
    "                training=self.is_training, name='acceleration')\n",
    "\n",
    "            action = tf.concat(\n",
    "                [steering, acceleration], name='action', axis=1)\n",
    "\n",
    "            weights = tf.get_collection(\n",
    "                tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope)\n",
    "\n",
    "        return action, weights, state\n",
    "\n",
    "    def _create_train(self):\n",
    "        action_gradient = tf.placeholder(tf.float32, [None, self.action_size])\n",
    "        params_grad = tf.gradients(self.action, self.weights,\n",
    "                                   tf.negative(action_gradient))\n",
    "        grads = zip(params_grad, self.weights)\n",
    "        optimize = self.trainer.apply_gradients(grads)\n",
    "        return optimize, action_gradient\n",
    "\n",
    "    def predict(self, sess, states):\n",
    "        self.is_training = False\n",
    "        return sess.run(self.action, feed_dict={self.state: states})\n",
    "\n",
    "    def target_predict(self, sess, states):\n",
    "        self.is_training = False\n",
    "        return sess.run(\n",
    "            self.target_action,\n",
    "            feed_dict={self.target_state: states})\n",
    "\n",
    "    def train(self, sess, states, action_grads):\n",
    "        self.training = True\n",
    "        sess.run(\n",
    "            self.optimize,\n",
    "            feed_dict={\n",
    "                self.state: states, self.action_gradient: action_grads})\n",
    "\n",
    "\n",
    "class A3CNetwork(Network):\n",
    "\n",
    "    def __init__(self, state_size, action_size, trainer, scope):\n",
    "        super(A3CNetwork, self).__init__(\n",
    "            state_size, action_size, trainer)\n",
    "        self.scope = scope\n",
    "        self.is_training = False\n",
    "        self._create_network()\n",
    "        if self.scope != 'global':\n",
    "            self._create_train()\n",
    "\n",
    "    @staticmethod\n",
    "    def update_target_graph(from_scope, to_scope):\n",
    "        from_vars = tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)\n",
    "        to_vars = tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)\n",
    "\n",
    "        op_holder = []\n",
    "        for from_var, to_var in zip(from_vars, to_vars):\n",
    "            op_holder.append(to_var.assign(from_var))\n",
    "\n",
    "        return op_holder\n",
    "\n",
    "    def _create_network(self):\n",
    "        with tf.variable_scope(self.scope):\n",
    "            # Input and visual encoding layers\n",
    "            self.inputs = tf.placeholder(\n",
    "                shape=[None, self.state_size], dtype=tf.float32)\n",
    "\n",
    "            s_layer1 = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=self.inputs, activation=tf.nn.relu,\n",
    "                    units=A3CNetwork.HIDDEN1_UNITS),\n",
    "                training=self.is_training, name='s_layer_1')\n",
    "\n",
    "            s_layer2 = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=s_layer1, activation=tf.nn.relu,\n",
    "                    units=A3CNetwork.HIDDEN2_UNITS),\n",
    "                training=self.is_training, name='s_layer_2')\n",
    "\n",
    "            # Output layers for policy and value estimations\n",
    "            self.policy_mu = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(\n",
    "                    inputs=s_layer2, units=2, activation=tf.nn.tanh),\n",
    "                training=self.is_training, name='policy_mu')\n",
    "\n",
    "            self.policy_sd = tf.clip_by_value(\n",
    "                tf.layers.batch_normalization(\n",
    "                    tf.layers.dense(\n",
    "                        inputs=s_layer2, units=2, activation=tf.nn.softplus),\n",
    "                    training=self.is_training),\n",
    "                [0.05]*self.action_size, [0.25]*self.action_size,\n",
    "                name='policy_sd')\n",
    "\n",
    "            self.value = tf.layers.batch_normalization(\n",
    "                tf.layers.dense(inputs=s_layer2, units=1),\n",
    "                training=self.is_training, name='value')\n",
    "\n",
    "            self.normal_dist = tf.contrib.distributions.Normal(\n",
    "                self.policy_mu, self.policy_sd, name='normal_dist')\n",
    "\n",
    "            self.action = tf.clip_by_value(\n",
    "                self.normal_dist.sample(1),\n",
    "                [-1.0]*self.action_size, [1.0]*self.action_size,\n",
    "                name='action')\n",
    "\n",
    "    def _create_train(self):\n",
    "        with tf.variable_scope(self.scope):\n",
    "            self.actions = tf.placeholder(\n",
    "                shape=[None, self.action_size], dtype=tf.float32,\n",
    "                name='actions')\n",
    "            self.target_v = tf.placeholder(\n",
    "                shape=[None], dtype=tf.float32, name='target_v')\n",
    "            self.advantages = tf.placeholder(\n",
    "                shape=[None], dtype=tf.float32, name='advantages')\n",
    "\n",
    "            log_prob = self.normal_dist.log_prob(self.actions)\n",
    "            exp_v = tf.transpose(\n",
    "                tf.multiply(tf.transpose(log_prob), self.advantages))\n",
    "            entropy = self.normal_dist.entropy()\n",
    "            exp_v = 0.01 * entropy + exp_v\n",
    "            self.policy_loss = tf.reduce_sum(-exp_v)\n",
    "\n",
    "            self.value_loss = 0.5 * tf.reduce_sum(\n",
    "                tf.square(self.target_v - tf.reshape(self.value, [-1])))\n",
    "\n",
    "            self.loss = 0.5*self.value_loss + self.policy_loss\n",
    "\n",
    "            local_vars = tf.get_collection(\n",
    "                tf.GraphKeys.TRAINABLE_VARIABLES, self.scope)\n",
    "\n",
    "            self.gradients = tf.gradients(self.loss, local_vars)\n",
    "            self.var_norms = tf.global_norm(local_vars)\n",
    "\n",
    "            grads, self.grad_norms = tf.clip_by_global_norm(\n",
    "                self.gradients, 40.0)\n",
    "\n",
    "            global_vars = tf.get_collection(\n",
    "                tf.GraphKeys.TRAINABLE_VARIABLES, 'global')\n",
    "            self.apply_grads = self.trainer.apply_gradients(\n",
    "                zip(grads, global_vars))\n",
    "\n",
    "    def predict(self, sess, state):\n",
    "        action = sess.run(\n",
    "            self.action,\n",
    "            feed_dict={self.inputs: [state]})\n",
    "        return action[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ddpg.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from networks import ActorNetwork, CriticNetwork\n",
    "from gym_torcs_docker import TorcsDockerEnv, obs_to_state\n",
    "from numpy.random import seed, randn\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        # Randomly sample batch_size examples\n",
    "        if self.num_experiences < batch_size:\n",
    "            return random.sample(self.buffer, self.num_experiences)\n",
    "        else:\n",
    "            return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, new_state, done):\n",
    "        experience = (state, action, reward, new_state, done)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_experiences += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0\n",
    "\n",
    "\n",
    "class DDPG(object):\n",
    "\n",
    "    def __init__(\n",
    "            self, docker_client, name='worker', port=3101,\n",
    "            model_path='../models/ddpg', log_path='../logs/ddpg'):\n",
    "\n",
    "        self.state_size = 29\n",
    "        self.action_size = 2\n",
    "\n",
    "        self.docker_client = docker_client\n",
    "\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.001  # Target Network HyperParameters\n",
    "        self.lra = 0.0001  # Learning rate for Actor\n",
    "        self.lrc = 0.001  # Lerning rate for Critic\n",
    "        seed(6486)\n",
    "\n",
    "        self.explore = 100000.\n",
    "        self.episode_count = 2000\n",
    "        self.max_steps = 10000\n",
    "        self.epsilon = 1\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.port = port\n",
    "        self.name = name\n",
    "\n",
    "        if not os.path.exists(self.model_path):\n",
    "                os.makedirs(self.model_path)\n",
    "\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.summary_writer = tf.summary.FileWriter(log_path)\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            self.state_size, self.action_size,\n",
    "            tf.train.AdamOptimizer(self.lra), self.tau)\n",
    "\n",
    "        self.critic = CriticNetwork(\n",
    "            self.state_size, self.action_size,\n",
    "            tf.train.AdamOptimizer(self.lrc), self.tau)\n",
    "\n",
    "        self.buff = ReplayBuffer(self.buffer_size)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self._create_summary()\n",
    "\n",
    "    def _create_summary(self):\n",
    "        with tf.name_scope('summary'):\n",
    "            self.loss_summary_op = tf.summary.scalar(\n",
    "                'loss', self.critic.loss, collections=['loss'])\n",
    "\n",
    "            self.reward_ph = tf.placeholder(\n",
    "                shape=[None, ], name='reward', dtype=tf.float32)\n",
    "            self.target_q_values_ph = tf.placeholder(\n",
    "                shape=[None, self.action_size], name='target_q_values',\n",
    "                dtype=tf.float32)\n",
    "            self.y_t_ph = tf.placeholder(\n",
    "                shape=[None, self.action_size], name='target_y_t',\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            tf.summary.scalar(\n",
    "                'reward', tf.reduce_mean(\n",
    "                    self.reward_ph), collections=['reward'])\n",
    "            tf.summary.scalar(\n",
    "                'target_q_values', tf.reduce_mean(self.target_q_values_ph),\n",
    "                collections=['reward'])\n",
    "            tf.summary.scalar(\n",
    "                'y_t', tf.reduce_mean(self.y_t_ph), collections=['reward'])\n",
    "\n",
    "            self.reward_summary_op = tf.summary.merge_all('reward')\n",
    "\n",
    "    @staticmethod\n",
    "    def addOUNoise(a, epsilon):\n",
    "\n",
    "        def ou_func(x, mu, theta, sigma):\n",
    "            return theta * (mu - x) + sigma * randn(1)\n",
    "\n",
    "        a_new = np.zeros(np.shape(a))\n",
    "        noise = np.zeros(np.shape(a))\n",
    "\n",
    "        noise[0] = (max(epsilon, 0) * ou_func(a[0], 0.0, 0.60, 0.30))\n",
    "        noise[1] = (max(epsilon, 0) * ou_func(a[1], 0.2, 1.00, 0.10))\n",
    "\n",
    "        a_new[0] = a[0] + noise[0]\n",
    "        a_new[1] = a[1] + noise[1]\n",
    "\n",
    "        return a_new\n",
    "\n",
    "    def train(self, track_name='', check_stuck=True):\n",
    "\n",
    "        all_steps = 0\n",
    "\n",
    "        if track_name == '':\n",
    "            env = TorcsDockerEnv(\n",
    "                self.docker_client, self.name, self.port, training=True)\n",
    "        else:\n",
    "            env = TorcsDockerEnv(\n",
    "                self.docker_client, self.name, self.port,\n",
    "                track_name=track_name)\n",
    "\n",
    "        with tf.Session(config=self.config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for i in range(self.episode_count):\n",
    "\n",
    "                recent_rewards = np.ones(1000) * 1e9\n",
    "                print(\"Episode : \" + str(i) + \" Replay Buffer \"\n",
    "                      + str(self.buff.count()))\n",
    "\n",
    "                if np.mod(i, 3) == 0:\n",
    "                    observation = env.reset(relaunch=True)\n",
    "                else:\n",
    "                    observation = env.reset()\n",
    "\n",
    "                state_t = obs_to_state(observation)\n",
    "                total_reward = 0\n",
    "\n",
    "                for j in range(self.max_steps):\n",
    "                    loss = 0\n",
    "                    self.epsilon -= 1.0 / self.explore\n",
    "\n",
    "                    action_t = self.actor.predict(\n",
    "                        sess, state_t.reshape(1, state_t.shape[0]))\n",
    "\n",
    "                    observation, reward_t, done, _ = env.step(\n",
    "                        DDPG.addOUNoise(action_t[0], self.epsilon))\n",
    "                    state_t1 = obs_to_state(observation)\n",
    "\n",
    "                    recent_rewards[j % 1000] = reward_t\n",
    "\n",
    "                    if (check_stuck and np.median(recent_rewards) < 1.0\n",
    "                            and i/self.episode_count < 0.5):\n",
    "                        break\n",
    "\n",
    "                    self.buff.add(\n",
    "                        state_t, action_t[0], reward_t, state_t1, done)\n",
    "                    batch = self.buff.getBatch(self.batch_size)\n",
    "                    states = np.asarray([e[0] for e in batch])\n",
    "                    actions = np.asarray([e[1] for e in batch])\n",
    "                    rewards = np.asarray([e[2] for e in batch])\n",
    "                    new_states = np.asarray([e[3] for e in batch])\n",
    "                    dones = np.asarray([e[4] for e in batch])\n",
    "                    y_t = np.asarray([e[1] for e in batch])\n",
    "\n",
    "                    target_q_values = self.critic.target_predict(\n",
    "                        sess, new_states,\n",
    "                        self.actor.target_predict(sess, new_states))\n",
    "\n",
    "                    for k in range(len(batch)):\n",
    "                        if dones[k]:\n",
    "                            y_t[k] = rewards[k]\n",
    "                        else:\n",
    "                            y_t[k] = (\n",
    "                                rewards[k] + self.gamma * target_q_values[k])\n",
    "\n",
    "                    loss += self.critic.train(sess, y_t, states, actions)\n",
    "                    actions_for_grad = self.actor.predict(sess, states)\n",
    "                    grads = self.critic.gradients(\n",
    "                        sess, states, actions_for_grad)\n",
    "                    self.actor.train(sess, states, grads)\n",
    "                    self.actor.target_train(sess)\n",
    "                    self.critic.target_train(sess)\n",
    "\n",
    "                    all_steps += 1\n",
    "\n",
    "                    if j % 50:\n",
    "\n",
    "                        loss_summary, reward_summary = sess.run(\n",
    "                            [self.loss_summary_op,\n",
    "                             self.reward_summary_op],\n",
    "                            feed_dict={\n",
    "                                self.critic.expected_critic: y_t,\n",
    "                                self.critic.state: states,\n",
    "                                self.critic.action: actions,\n",
    "                                self.reward_ph: rewards,\n",
    "                                self.target_q_values_ph: target_q_values,\n",
    "                                self.y_t_ph: y_t})\n",
    "\n",
    "                        self.summary_writer.add_summary(\n",
    "                            loss_summary, all_steps)\n",
    "                        self.summary_writer.add_summary(\n",
    "                            reward_summary, all_steps)\n",
    "                        self.summary_writer.flush()\n",
    "\n",
    "                    total_reward += reward_t\n",
    "                    state_t = state_t1\n",
    "                    print(\n",
    "                        \"Episode\", i, \"Step\", all_steps, \"Action\",\n",
    "                        action_t, \"Reward\", reward_t, \"Loss\", loss)\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                print(\"TOTAL REWARD @ \" + str(i) + \"-th Episode  : Reward \" +\n",
    "                      str(total_reward))\n",
    "                print(\"Total Step: \" + str(all_steps))\n",
    "                print(\"\")\n",
    "\n",
    "                if np.mod(i, 50) == 0:\n",
    "                    self.saver.save(\n",
    "                        sess, self.model_path+'/model-{:d}.cptk'.format(i))\n",
    "        env.end()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import docker\n",
    "\n",
    "    docker_client = docker.from_env()\n",
    "\n",
    "    ddpg = DDPG(\n",
    "        docker_client, 3101, '../models/ddpg_gtrack1', '../logs/ddpg_gtrack1')\n",
    "    ddpg.train('g-track-1')\n",
    "\n",
    "    ddpg = DDPG(\n",
    "        docker_client, 3101, '../models/ddpg_traintracks',\n",
    "        '../logs/ddpg_traintracks')\n",
    "    ddpg.train()\n",
    "\n",
    "    ddpg = DDPG(\n",
    "        docker_client, 3101, '../models/ddpg_gtrack1_nostuck',\n",
    "        '../logs/ddpg_gtrack1_nostuck')\n",
    "    ddpg.train('g-track-1', False)\n",
    "\n",
    "    ddpg.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 214, in _raise_for_status\n",
      "    response.raise_for_status()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/requests/models.py\", line 909, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 404 Client Error: Not Found for url: http+docker://localunixsocket/v1.26/containers/worker_0/json\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-5-07433b9e6a66>\", line 286, in <lambda>\n",
      "    saver)))\n",
      "  File \"<ipython-input-5-07433b9e6a66>\", line 77, in work\n",
      "    self.docker_client, self.name, self.docker_port, training=True)\n",
      "  File \"/root/rl_torcs/src/gym_torcs_docker.py\", line 39, in __init__\n",
      "    self.container = self._start_docker()\n",
      "  File \"/root/rl_torcs/src/gym_torcs_docker.py\", line 78, in _start_docker\n",
      "    return self.docker_client.containers.get(self.name)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/docker/models/containers.py\", line 723, in get\n",
      "    resp = self.client.api.inspect_container(container_id)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/docker/utils/decorators.py\", line 21, in wrapped\n",
      "    return f(self, resource_id, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/docker/api/container.py\", line 748, in inspect_container\n",
      "    self._get(self._url(\"/containers/{0}/json\", container)), True\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 220, in _result\n",
      "    self._raise_for_status(response)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/docker/api/client.py\", line 216, in _raise_for_status\n",
      "    raise create_api_error_from_http_exception(e)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/docker/errors.py\", line 30, in create_api_error_from_http_exception\n",
      "    raise cls(e, response=response, explanation=explanation)\n",
      "docker.errors.NotFound: 404 Client Error: Not Found (\"No such container: worker_0\")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "import os\n",
    "import threading\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.signal\n",
    "\n",
    "from time import sleep\n",
    "from gym_torcs_docker import TorcsDockerEnv, obs_to_state\n",
    "from networks import A3CNetwork\n",
    "\n",
    "\n",
    "class Worker(object):\n",
    "\n",
    "    def __init__(self, s_size, action_size, trainer, number, global_episodes,\n",
    "                 docker_client, docker_port, modeldir, logdir):\n",
    "\n",
    "        self.s_size = s_size\n",
    "        self.action_size = action_size\n",
    "        self.number = number\n",
    "        self.trainer = trainer\n",
    "        self.global_episodes = global_episodes\n",
    "        self.docker_client = docker_client\n",
    "        self.modeldir = modeldir\n",
    "        self.logdir = logdir\n",
    "\n",
    "        self.name = 'worker_'+str(self.number)\n",
    "        self.docker_port = docker_port\n",
    "\n",
    "        self.increment = self.global_episodes.assign_add(1)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_values = []\n",
    "        self.summary_writer = tf.summary.FileWriter(\n",
    "            self.logdir + '/train_' + str(self.number))\n",
    "\n",
    "        self.local_AC = A3CNetwork(\n",
    "            self.s_size, self.action_size, self.trainer, self.name)\n",
    "        self.update_local_ops = A3CNetwork.update_target_graph(\n",
    "            'global', self.name)\n",
    "\n",
    "    def train(self, rollout, sess, gamma, bootstrap_value):\n",
    "        def discount(x, gamma):\n",
    "            return scipy.signal.lfilter(\n",
    "                [1], [1, -gamma], x[::-1], axis=0)[::-1]\n",
    "\n",
    "        self.local_AC.is_training = True\n",
    "        rollout = np.array(rollout)\n",
    "        observations = rollout[:, 0]\n",
    "        actions = np.stack(rollout[:, 1], 0)[0][0]\n",
    "        rewards = rollout[:, 2]\n",
    "        values = rollout[:, 5]\n",
    "        self.rewards_plus = np.asarray(\n",
    "            rewards.tolist() + [bootstrap_value])\n",
    "\n",
    "        discounted_rewards = discount(self.rewards_plus, gamma)[:-1]\n",
    "        self.value_plus = np.asarray(values.tolist() + [bootstrap_value])\n",
    "        advantages = (\n",
    "            rewards + gamma * self.value_plus[1:] - self.value_plus[:-1])\n",
    "        feed_dict = {self.local_AC.target_v: discounted_rewards,\n",
    "                     self.local_AC.actions: actions,\n",
    "                     self.local_AC.inputs: np.vstack(observations),\n",
    "                     self.local_AC.advantages: advantages}\n",
    "        value_loss, policy_loss, gradient_norm, value_norm, _ = sess.run(\n",
    "            [self.local_AC.value_loss, self.local_AC.policy_loss,\n",
    "             self.local_AC.grad_norms, self.local_AC.var_norms,\n",
    "             self.local_AC.apply_grads],\n",
    "            feed_dict=feed_dict)\n",
    "        self.local_AC.is_training = False\n",
    "\n",
    "        return (value_loss/len(rollout), policy_loss/len(rollout),\n",
    "                gradient_norm, value_norm)\n",
    "\n",
    "    def work(self, max_episode_length, gamma, sess, coord, saver):\n",
    "        self.local_AC.is_training = False\n",
    "        env = TorcsDockerEnv(\n",
    "            self.docker_client, self.name, self.docker_port, training=True)\n",
    "\n",
    "        episode_count = sess.run(self.global_episodes)\n",
    "        total_steps = 0\n",
    "        print(\"Starting {}\".format(self.name))\n",
    "\n",
    "        with sess.as_default(), sess.graph.as_default():\n",
    "            while not coord.should_stop():\n",
    "                sess.run(self.update_local_ops)\n",
    "                episode_buffer = []\n",
    "                episode_values = []\n",
    "                episode_frames = []\n",
    "                episode_reward = 0\n",
    "                episode_step_count = 0\n",
    "\n",
    "                # reset docker every third episode\n",
    "                local_episodes = 0\n",
    "                if np.mod(local_episodes, 3) == 0:\n",
    "                    observation = env.reset(relaunch=True)\n",
    "                else:\n",
    "                    observation = env.reset()\n",
    "                state_t = obs_to_state(observation)\n",
    "                done = False\n",
    "\n",
    "                epsilon = 1\n",
    "\n",
    "                while not done:\n",
    "\n",
    "                    action_t, value_t = sess.run(\n",
    "                        [self.local_AC.action, self.local_AC.value],\n",
    "                        feed_dict={self.local_AC.inputs: [state_t]})\n",
    "\n",
    "                    epsilon -= 1.0 / max_episode_length\n",
    "\n",
    "                    observation, reward_t, done, _ = env.step(action_t[0][0])\n",
    "\n",
    "                    if not done:\n",
    "                        state_t1 = obs_to_state(observation)\n",
    "                        episode_frames.append(state_t1)\n",
    "                    else:\n",
    "                        state_t1 = state_t\n",
    "\n",
    "                    episode_buffer.append(\n",
    "                        [state_t, action_t, reward_t, state_t1, done,\n",
    "                         value_t[0, 0]])\n",
    "                    episode_values.append(value_t[0, 0])\n",
    "\n",
    "                    episode_reward += reward_t\n",
    "\n",
    "                    state_t = state_t1\n",
    "                    total_steps += 1\n",
    "                    episode_step_count += 1\n",
    "\n",
    "                    if total_steps % 20:\n",
    "                        print(\n",
    "                            \"Worker\", self.name,\n",
    "                            \"Episode\", episode_count, \"Step\",\n",
    "                            episode_step_count, \"Total_Steps\",\n",
    "                            total_steps, \"Action\", action_t[0][0],\n",
    "                            \"Reward\", reward_t)\n",
    "                        summary = tf.Summary()\n",
    "                        summary.value.add(\n",
    "                            tag='summary/reward_1',\n",
    "                            simple_value=float(reward_t))\n",
    "                        self.summary_writer.add_summary(\n",
    "                            summary, total_steps)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "\n",
    "                    if (len(episode_buffer) == 30 and not done\n",
    "                            and episode_step_count != max_episode_length-1):\n",
    "\n",
    "                        value_t1 = sess.run(\n",
    "                            self.local_AC.value,\n",
    "                            feed_dict={self.local_AC.inputs: [state_t]})[0, 0]\n",
    "\n",
    "                        (value_loss, policy_loss, gradient_norm,\n",
    "                            variable_norm) = self.train(\n",
    "                                episode_buffer, sess, gamma, value_t1)\n",
    "                        episode_buffer = []\n",
    "                        sess.run(self.update_local_ops)\n",
    "                    if (done or episode_step_count != max_episode_length):\n",
    "                        break\n",
    "\n",
    "                local_episodes += 1\n",
    "                self.episode_rewards.append(episode_reward)\n",
    "                self.episode_lengths.append(episode_step_count)\n",
    "                self.episode_mean_values.append(\n",
    "                    np.mean(episode_values))\n",
    "\n",
    "                if len(episode_buffer) != 0:\n",
    "                    (value_loss, policy_loss, gradient_norm,\n",
    "                     variable_norm) = self.train(\n",
    "                        episode_buffer, sess, gamma, 0.0)\n",
    "\n",
    "                if episode_count != 0:\n",
    "                    if (self.name == 'worker_0'):\n",
    "                        saver.save(\n",
    "                            sess,\n",
    "                            os.path.join(self.modeldir,\n",
    "                                         'model-{:d}.cptk'.format(\n",
    "                                             episode_count)))\n",
    "\n",
    "                    mean_reward = np.mean(self.episode_rewards[-5:])\n",
    "                    mean_length = np.mean(self.episode_lengths[-5:])\n",
    "                    mean_value = np.mean(self.episode_mean_values[-5:])\n",
    "\n",
    "                    print(\n",
    "                        \"Worker\", self.name, \"Episode\", episode_count,\n",
    "                        \"Reward\", mean_reward, \"value_Loss\", value_loss,\n",
    "                        \"policy_loss\", policy_loss)\n",
    "\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(\n",
    "                        tag='Perf/Reward',\n",
    "                        simple_value=float(mean_reward))\n",
    "                    summary.value.add(\n",
    "                        tag='Perf/Length',\n",
    "                        simple_value=float(mean_length))\n",
    "                    summary.value.add(\n",
    "                        tag='Perf/Value',\n",
    "                        simple_value=float(mean_value))\n",
    "                    summary.value.add(\n",
    "                        tag='Losses/Value Loss',\n",
    "                        simple_value=float(value_loss))\n",
    "                    summary.value.add(\n",
    "                        tag='Losses/Policy Loss',\n",
    "                        simple_value=float(policy_loss))\n",
    "                    summary.value.add(\n",
    "                        tag='Losses/Grad Norm',\n",
    "                        simple_value=float(gradient_norm))\n",
    "                    summary.value.add(\n",
    "                        tag='Losses/Var Norm',\n",
    "                        simple_value=float(variable_norm))\n",
    "\n",
    "                    self.summary_writer.add_summary(\n",
    "                        summary, episode_count)\n",
    "\n",
    "                    self.summary_writer.flush()\n",
    "\n",
    "                if self.name == 'worker_0':\n",
    "                    sess.run(self.increment)\n",
    "                episode_count += 1\n",
    "        env.end()\n",
    "\n",
    "\n",
    "class A3C(object):\n",
    "\n",
    "    def __init__(\n",
    "            self, docker_client, docker_start_port=3101,\n",
    "            modeldir='../models/a3c', logdir='../logs/a3c'):\n",
    "\n",
    "        self.docker_client = docker_client\n",
    "\n",
    "        self.docker_start_port = docker_start_port\n",
    "\n",
    "        self.max_episode_length = 4000\n",
    "        self.gamma = .99\n",
    "        self.logdir = logdir\n",
    "        self.modeldir = modeldir\n",
    "        self.state_size = 29\n",
    "        self.action_size = 2\n",
    "\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.global_episodes = tf.Variable(\n",
    "                0, dtype=tf.int32, name='global_episodes', trainable=False)\n",
    "\n",
    "        if not os.path.exists(self.modeldir):\n",
    "                os.makedirs(self.modeldir)\n",
    "\n",
    "    def train(self, num_workers, load_model=False):\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "\n",
    "            trainer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "            master_network = A3CNetwork(\n",
    "                self.state_size, self.action_size, None, 'global')\n",
    "\n",
    "            workers = []\n",
    "            for i in range(num_workers):\n",
    "                workers.append(\n",
    "                    Worker(\n",
    "                        self.state_size, self.action_size, trainer, i,\n",
    "                        self.global_episodes, self.docker_client,\n",
    "                        self.docker_start_port + i,\n",
    "                        self.modeldir, self.logdir))\n",
    "\n",
    "            saver = tf.train.Saver(max_to_keep=5)\n",
    "\n",
    "        with tf.Session(config=self.config) as sess:\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "\n",
    "            if load_model:\n",
    "                print('Loading Model...')\n",
    "                ckpt = tf.train.get_checkpoint_state(self.model_path)\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            else:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            worker_threads = []\n",
    "            for worker in workers:\n",
    "                t = threading.Thread(\n",
    "                    target=(\n",
    "                        lambda: worker.work(\n",
    "                            self.max_episode_length, self.gamma, sess, coord,\n",
    "                            saver)))\n",
    "                t.start()\n",
    "                sleep(0.5)\n",
    "                worker_threads.append(t)\n",
    "            coord.join(worker_threads)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import docker\n",
    "\n",
    "    docker_client = docker.from_env()\n",
    "\n",
    "    a3c = A3C(docker_client)\n",
    "    a3c.train(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load ddpg.py\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import deque\n",
    "from networks import ActorNetwork, CriticNetwork\n",
    "from gym_torcs_docker import TorcsDockerEnv, obs_to_state\n",
    "from numpy.random import seed, randn\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.num_experiences = 0\n",
    "        self.buffer = deque()\n",
    "\n",
    "    def getBatch(self, batch_size):\n",
    "        # Randomly sample batch_size examples\n",
    "        if self.num_experiences < batch_size:\n",
    "            return random.sample(self.buffer, self.num_experiences)\n",
    "        else:\n",
    "            return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def size(self):\n",
    "        return self.buffer_size\n",
    "\n",
    "    def add(self, state, action, reward, new_state, done):\n",
    "        experience = (state, action, reward, new_state, done)\n",
    "        if self.num_experiences < self.buffer_size:\n",
    "            self.buffer.append(experience)\n",
    "            self.num_experiences += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def count(self):\n",
    "        # if buffer is full, return buffer size\n",
    "        # otherwise, return experience counter\n",
    "        return self.num_experiences\n",
    "\n",
    "    def erase(self):\n",
    "        self.buffer = deque()\n",
    "        self.num_experiences = 0\n",
    "\n",
    "\n",
    "class DDPG(object):\n",
    "\n",
    "    def __init__(\n",
    "            self, docker_client, name='worker', port=3101,\n",
    "            model_path='../models/ddpg', log_path='../logs/ddpg'):\n",
    "\n",
    "        self.state_size = 29\n",
    "        self.action_size = 2\n",
    "\n",
    "        self.docker_client = docker_client\n",
    "\n",
    "        self.buffer_size = 100000\n",
    "        self.batch_size = 32\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.001  # Target Network HyperParameters\n",
    "        self.lra = 0.0001  # Learning rate for Actor\n",
    "        self.lrc = 0.001  # Lerning rate for Critic\n",
    "        seed(6486)\n",
    "\n",
    "        self.explore = 100000.\n",
    "        self.episode_count = 2000\n",
    "        self.max_steps = 10000\n",
    "        self.epsilon = 1\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.port = port\n",
    "        self.name = name\n",
    "\n",
    "        if not os.path.exists(self.model_path):\n",
    "                os.makedirs(self.model_path)\n",
    "\n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.summary_writer = tf.summary.FileWriter(log_path)\n",
    "\n",
    "        self.actor = ActorNetwork(\n",
    "            self.state_size, self.action_size,\n",
    "            tf.train.AdamOptimizer(self.lra), self.tau)\n",
    "\n",
    "        self.critic = CriticNetwork(\n",
    "            self.state_size, self.action_size,\n",
    "            tf.train.AdamOptimizer(self.lrc), self.tau)\n",
    "\n",
    "        self.buff = ReplayBuffer(self.buffer_size)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self._create_summary()\n",
    "\n",
    "    def _create_summary(self):\n",
    "        with tf.name_scope('summary'):\n",
    "            self.loss_summary_op = tf.summary.scalar(\n",
    "                'loss', self.critic.loss, collections=['loss'])\n",
    "\n",
    "            self.reward_ph = tf.placeholder(\n",
    "                shape=[None, ], name='reward', dtype=tf.float32)\n",
    "            self.target_q_values_ph = tf.placeholder(\n",
    "                shape=[None, self.action_size], name='target_q_values',\n",
    "                dtype=tf.float32)\n",
    "            self.y_t_ph = tf.placeholder(\n",
    "                shape=[None, self.action_size], name='target_y_t',\n",
    "                dtype=tf.float32)\n",
    "\n",
    "            tf.summary.scalar(\n",
    "                'reward', tf.reduce_mean(\n",
    "                    self.reward_ph), collections=['reward'])\n",
    "            tf.summary.scalar(\n",
    "                'target_q_values', tf.reduce_mean(self.target_q_values_ph),\n",
    "                collections=['reward'])\n",
    "            tf.summary.scalar(\n",
    "                'y_t', tf.reduce_mean(self.y_t_ph), collections=['reward'])\n",
    "\n",
    "            self.reward_summary_op = tf.summary.merge_all('reward')\n",
    "\n",
    "    @staticmethod\n",
    "    def addOUNoise(a, epsilon):\n",
    "\n",
    "        def ou_func(x, mu, theta, sigma):\n",
    "            return theta * (mu - x) + sigma * randn(1)\n",
    "\n",
    "        a_new = np.zeros(np.shape(a))\n",
    "        noise = np.zeros(np.shape(a))\n",
    "\n",
    "        noise[0] = (max(epsilon, 0) * ou_func(a[0], 0.0, 0.60, 0.30))\n",
    "        noise[1] = (max(epsilon, 0) * ou_func(a[1], 0.2, 1.00, 0.10))\n",
    "\n",
    "        a_new[0] = a[0] + noise[0]\n",
    "        a_new[1] = a[1] + noise[1]\n",
    "\n",
    "        return a_new\n",
    "\n",
    "    def train(self, track_name='', check_stuck=True):\n",
    "\n",
    "        all_steps = 0\n",
    "\n",
    "        if track_name == '':\n",
    "            env = TorcsDockerEnv(\n",
    "                self.docker_client, self.name, self.port, training=True)\n",
    "        else:\n",
    "            env = TorcsDockerEnv(\n",
    "                self.docker_client, self.name, self.port,\n",
    "                track_name=track_name)\n",
    "\n",
    "        with tf.Session(config=self.config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for i in range(self.episode_count):\n",
    "\n",
    "                recent_rewards = np.ones(1000) * 1e9\n",
    "                print(\"Episode : \" + str(i) + \" Replay Buffer \"\n",
    "                      + str(self.buff.count()))\n",
    "\n",
    "                if np.mod(i, 3) == 0:\n",
    "                    observation = env.reset(relaunch=True)\n",
    "                else:\n",
    "                    observation = env.reset()\n",
    "\n",
    "                state_t = obs_to_state(observation)\n",
    "                total_reward = 0\n",
    "\n",
    "                for j in range(self.max_steps):\n",
    "                    loss = 0\n",
    "                    self.epsilon -= 1.0 / self.explore\n",
    "\n",
    "                    action_t = self.actor.predict(\n",
    "                        sess, state_t.reshape(1, state_t.shape[0]))\n",
    "\n",
    "                    observation, reward_t, done, _ = env.step(\n",
    "                        DDPG.addOUNoise(action_t[0], self.epsilon))\n",
    "                    state_t1 = obs_to_state(observation)\n",
    "\n",
    "                    recent_rewards[j % 1000] = reward_t\n",
    "\n",
    "                    if (check_stuck and np.median(recent_rewards) < 1.0\n",
    "                            and i/self.episode_count < 0.5):\n",
    "                        break\n",
    "\n",
    "                    self.buff.add(\n",
    "                        state_t, action_t[0], reward_t, state_t1, done)\n",
    "                    batch = self.buff.getBatch(self.batch_size)\n",
    "                    states = np.asarray([e[0] for e in batch])\n",
    "                    actions = np.asarray([e[1] for e in batch])\n",
    "                    rewards = np.asarray([e[2] for e in batch])\n",
    "                    new_states = np.asarray([e[3] for e in batch])\n",
    "                    dones = np.asarray([e[4] for e in batch])\n",
    "                    y_t = np.asarray([e[1] for e in batch])\n",
    "\n",
    "                    target_q_values = self.critic.target_predict(\n",
    "                        sess, new_states,\n",
    "                        self.actor.target_predict(sess, new_states))\n",
    "\n",
    "                    for k in range(len(batch)):\n",
    "                        if dones[k]:\n",
    "                            y_t[k] = rewards[k]\n",
    "                        else:\n",
    "                            y_t[k] = (\n",
    "                                rewards[k] + self.gamma * target_q_values[k])\n",
    "\n",
    "                    loss += self.critic.train(sess, y_t, states, actions)\n",
    "                    actions_for_grad = self.actor.predict(sess, states)\n",
    "                    grads = self.critic.gradients(\n",
    "                        sess, states, actions_for_grad)\n",
    "                    self.actor.train(sess, states, grads)\n",
    "                    self.actor.target_train(sess)\n",
    "                    self.critic.target_train(sess)\n",
    "\n",
    "                    all_steps += 1\n",
    "\n",
    "                    if j % 50:\n",
    "\n",
    "                        loss_summary, reward_summary = sess.run(\n",
    "                            [self.loss_summary_op,\n",
    "                             self.reward_summary_op],\n",
    "                            feed_dict={\n",
    "                                self.critic.expected_critic: y_t,\n",
    "                                self.critic.state: states,\n",
    "                                self.critic.action: actions,\n",
    "                                self.reward_ph: rewards,\n",
    "                                self.target_q_values_ph: target_q_values,\n",
    "                                self.y_t_ph: y_t})\n",
    "\n",
    "                        self.summary_writer.add_summary(\n",
    "                            loss_summary, all_steps)\n",
    "                        self.summary_writer.add_summary(\n",
    "                            reward_summary, all_steps)\n",
    "                        self.summary_writer.flush()\n",
    "\n",
    "                    total_reward += reward_t\n",
    "                    state_t = state_t1\n",
    "                    print(\n",
    "                        \"Episode\", i, \"Step\", all_steps, \"Action\",\n",
    "                        action_t, \"Reward\", reward_t, \"Loss\", loss)\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                print(\"TOTAL REWARD @ \" + str(i) + \"-th Episode  : Reward \" +\n",
    "                      str(total_reward))\n",
    "                print(\"Total Step: \" + str(all_steps))\n",
    "                print(\"\")\n",
    "\n",
    "                if np.mod(i, 50) == 0:\n",
    "                    self.saver.save(\n",
    "                        sess, self.model_path+'/model-{:d}.cptk'.format(i))\n",
    "        env.end()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import docker\n",
    "\n",
    "    docker_client = docker.from_env()\n",
    "\n",
    "    ddpg = DDPG(\n",
    "        docker_client, 3101, '../models/ddpg_gtrack1', '../logs/ddpg_gtrack1')\n",
    "    ddpg.train('g-track-1')\n",
    "\n",
    "    ddpg = DDPG(\n",
    "        docker_client, 3101, '../models/ddpg_traintracks',\n",
    "        '../logs/ddpg_traintracks')\n",
    "    ddpg.train()\n",
    "\n",
    "    ddpg = DDPG(\n",
    "        docker_client, 3101, '../models/ddpg_gtrack1_nostuck',\n",
    "        '../logs/ddpg_gtrack1_nostuck')\n",
    "    ddpg.train('g-track-1', False)\n",
    "\n",
    "    ddpg.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
